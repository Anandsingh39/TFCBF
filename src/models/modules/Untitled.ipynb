{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673b5c08-efb7-436f-8463-6876e0e4cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c04463b-5834-4d3d-95ba-f84a30a0ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"pact_dataset_ctx16.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1efb44-ced5-44b0-85e1-f71d9c1833df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile 'pact_dataset_ctx16.npz' with keys: state_ctx, action_ctx, target_action, valid_mask, episode..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = np.load(\"pact_dataset_ctx16.npz\", allow_pickle=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd7a041-18bc-4971-b771-9e0544515cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 16, 38)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['state_ctx'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d066c607-2ae9-4a53-a925-232cd1a4cbc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        [-7.4633515e-01,  2.6657838e-01]],\n",
       "\n",
       "       [[ 0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        [-7.4633515e-01,  2.6657838e-01],\n",
       "        [-6.8383753e-01,  2.4425530e-01]],\n",
       "\n",
       "       [[ 0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00],\n",
       "        ...,\n",
       "        [-7.4633515e-01,  2.6657838e-01],\n",
       "        [-6.8383753e-01,  2.4425530e-01],\n",
       "        [-6.1303717e-01,  1.6914645e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.9440800e-03,  1.6478677e-03],\n",
       "        [-2.8699562e-03,  6.0646719e-04],\n",
       "        [-3.2943934e-03,  7.4626347e-05],\n",
       "        ...,\n",
       "        [-2.9486381e-03, -5.6967093e-04],\n",
       "        [-2.6967973e-03, -4.6328400e-04],\n",
       "        [-2.5319010e-03, -4.0341003e-04]],\n",
       "\n",
       "       [[-2.8699562e-03,  6.0646719e-04],\n",
       "        [-3.2943934e-03,  7.4626347e-05],\n",
       "        [ 8.1569672e-02, -6.0760111e-02],\n",
       "        ...,\n",
       "        [-2.6967973e-03, -4.6328400e-04],\n",
       "        [-2.5319010e-03, -4.0341003e-04],\n",
       "        [-2.4128556e-03, -3.6737768e-04]],\n",
       "\n",
       "       [[-3.2943934e-03,  7.4626347e-05],\n",
       "        [ 8.1569672e-02, -6.0760111e-02],\n",
       "        [-4.7013089e-02,  3.0672111e-02],\n",
       "        ...,\n",
       "        [-2.5319010e-03, -4.0341003e-04],\n",
       "        [-2.4128556e-03, -3.6737768e-04],\n",
       "        [-3.3637848e-02,  1.4022903e-02]]],\n",
       "      shape=(512, 16, 2), dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['action_ctx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29951c16-107e-4c38-81a4-fbeb48ef3ac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# adjust the import path to your project layout if needed\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_pact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PACTBase\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# PACTBase wraps PACTTokenizer internally and builds the GPT backbone. :contentReference[oaicite:6]{index=6}\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_pact_model\u001b[39m(state_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m38\u001b[39m, action_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, ctx_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     10\u001b[0m                      n_embd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, n_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, n_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# model_and_tokenizer.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# adjust the import path to your project layout if needed\n",
    "from src.models.modules.modeling_pact import PACTBase\n",
    "# PACTBase wraps PACTTokenizer internally and builds the GPT backbone. :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "def build_pact_model(state_dim=38, action_dim=2, ctx_tokens=16,\n",
    "                     n_embd=128, n_layer=4, n_head=8):\n",
    "    \"\"\"\n",
    "    ctx_tokens = number of tokens the transformer sees (must be even).\n",
    "    Because GPT block_size = 2 * seq_len, set seq_len = ctx_tokens // 2.  :contentReference[oaicite:7]{index=7}\n",
    "    \"\"\"\n",
    "    assert ctx_tokens % 2 == 0, \"ctx_tokens must be even\"\n",
    "    seq_len = ctx_tokens // 2\n",
    "\n",
    "    gpt_config = dict(\n",
    "        n_embd=n_embd, n_layer=n_layer, n_head=n_head,\n",
    "        embd_pdrop=0.1, resid_pdrop=0.1, attn_pdrop=0.1,\n",
    "        seq_len=seq_len\n",
    "    )\n",
    "\n",
    "    # Different tokenizers for \"state\" vs \"action\"\n",
    "    # state -> VectorStateTokenizer (mlp_state)\n",
    "    # action -> ActionTokenizer (mlp_action)\n",
    "    # The action tokenizer handles both continuous and discrete; we use \"continuous\" here. :contentReference[oaicite:8]{index=8} :contentReference[oaicite:9]{index=9}\n",
    "    input_config = {\n",
    "        \"state\": {\n",
    "            \"tokenizer\": \"mlp_state\",\n",
    "            \"input_type\": \"continuous\",\n",
    "            \"tokenizer_kwargs\": {\"state_dim\": state_dim, \"hidden\": [256, 256], \"use_ln\": True},\n",
    "        },\n",
    "        \"action\": {\n",
    "            \"tokenizer\": \"mlp_action\",\n",
    "            \"input_type\": \"continuous\",  # change to \"discrete\" if your actions are ids\n",
    "            \"tokenizer_kwargs\": {\"action_dim\": action_dim, \"hidden\": [128, 128], \"use_ln\": True},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    model = PACTBase(gpt_config=gpt_config, input_config=input_config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afb15f1-5c12-48dc-9528-20643d863ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01ma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_demo\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# points to your two-episode, 256-step dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m res \u001b[38;5;241m=\u001b[39m run_demo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpact_dataset.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                state_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                action_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaken_action\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# or \"nominal_action\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/PACT-main/src/models/modules/a.py:13\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Imports from your codebase\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# from src.models.modules.modeling_pact import PACTBase\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# from src.models.modules.tokenizer_pact import PACTTokenizer  # not strictly needed here, but handy for debugging\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodeling_pact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PACTBase\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtokenizer_pact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PACTTokenizer  \u001b[38;5;66;03m# not strictly needed here, but handy for debugging\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Config\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/PACT-main/src/models/modules/modeling_pact.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightningModule\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminGPT\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT, GPTConfig\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_pact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PACTTokenizer\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn \u001b[38;5;28;01mas\u001b[39;00m nn\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from a import run_demo\n",
    "\n",
    "# points to your two-episode, 256-step dataset\n",
    "res = run_demo(\"pact_dataset.npz\",\n",
    "               state_key=\"full_obs\",\n",
    "               action_key=\"taken_action\")   # or \"nominal_action\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f66a3-ed60-4e48-96d0-329739e48fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9601f-864d-4998-97bd-77cd41dd7abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a739bca-f7ec-4603-8b74-a45b06b33c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
